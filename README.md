# LongCA-bench

Long-Context Attention Benchmark

A unified framework to evaluate and analyze long-context attention for large language models. It supports dense and sparse kernels, distributed context-parallel mechanisms, and standardized data interfaces for fair comparison.

Conducts large-scale experiments across multiple mask patterns and ultra-long sequences (up to 512K tokens on 96 GPUs), providing insights into efficiency, scalability, and design trade-offs for LLM training.
